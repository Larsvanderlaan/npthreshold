---
title: "analyssi"
output: html_document
date: '2023-08-10'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Survival analysis

```{r}
library(data.table)
library(assertthat)


# Analysis parameters
subset_region <- "la"
subset_senior <- 0
country_strata <- c("la_nonsenior" = 181 , "la_senior" = 97, "US_nonsenior" = 109, "US_senior" = 61, "RSA_nonsenior" = 101)

## get reference time for survival analysis
tf <- country_strata[paste0(subset_region,"_", ifelse(subset_senior==1, "senior", "nonsenior") )]

# extract region-specific data
data <- setDT(fread(paste0("data/janssen_", subset_region, "_partA_data_processed_with_riskscore_hotdeckv4.csv")))
head(data)

# subset vaccine arm, per protocol, and in phase-two-sample (for day 29 marker)
data <- data[Trt==1]
data <- data[Perprotocol==1]
data <- data[TwophasesampIndD29==1]
data <- data[Senior == subset_senior]
assert_that(all(
  data$Trt == 1 
  & data$Perprotocol == 1
  & data$TwophasesampIndD29 == 1
  & data$Senior == subset_senior
)
)


# adjustment variables
weights <- "wt.D29" #"wt.D29start1"
covariates <- c("standardized_risk_score", "age.geq.65") #c("HighRiskInd", "Sex", "age.geq.65","standardized_risk_score")
failure_time <- "EventTimePrimaryD29"
event_type <- "EventIndPrimaryD29"
marker <- paste0("Day29pseudoneutid50", subset_region)


# make survival dataset
data_surv <- data[, c(covariates, failure_time, event_type, marker, weights), with = FALSE]
# discretize failure time into integers larger than one (quantile scale)
nbins <- 20
time_grid <- unique(quantile(data_surv[[failure_time]], seq(0,1, length = nbins+1), type = 1))
failure_time_discrete <- findInterval(data_surv[[failure_time]], time_grid, all.inside = TRUE)
tf_discrete <- findInterval(tf, time_grid, all.inside = TRUE)
data_surv[[failure_time]] <-failure_time_discrete

print(head(data_surv))
table(data_surv$EventTimePrimaryD29)
table(data_surv$EventIndPrimaryD29)

# TODO for nowo drop
print(table(is.na(data_surv[[weights]])))
data_surv <- data_surv[!is.na(data_surv[[weights]])]

table((is.na(unlist(data_surv))))
data_surv <- data_surv[!is.na(data_surv[["standardized_risk_score"]])]
```



```{r}
#library(npsurvival)
# marker threshold for analysis
library(future)
plan(multisession, workers = 6)
 

# learning ensemble for failure time hazard
sl_glm_oneway_t <- Lrnr_sl$new(learners = lapply(covariates, function(var) {
  var <- c(var, "treatment", "t")
  Lrnr_glm_fast$new(formula =  ~ . , covariates = var)
}), Lrnr_nnls$new())
sl_glm_twoway_t <- Lrnr_sl$new(learners = lapply(covariates, function(var) {
  var <- c(var, "treatment", "t")
  Lrnr_glm_fast$new(formula =  ~ . + t*treatment, covariates = var)
}), metalearner = Lrnr_nnls$new(), 
fold_fun = origami::folds_vfold, V = 5)

sl_gam_t <- Lrnr_sl$new(learners = lapply(covariates, function(var) {
  var <- c(var, "treatment", "t")
  Lrnr_gam$new(covariates = var)
}
), metalearner = Lrnr_nnls$new(),
fold_fun = origami::folds_vfold, V = 5)


Lrnr_KM <- Lrnr_hal9001$new(formula = ~  h(t, k = 20, pf = 0, s = 0), smoothness_orders = 0)
Lrnr_cox <- Lrnr_hal9001$new(formula = ~ h(., k =1, s = 1) + h(t, k = 20, pf = 0, s = 0), smoothness_orders = 1)


stack.failure <- Stack$new(
  sl_glm_oneway_t, # stacked one-way glms
  sl_glm_twoway_t, # stacked two-way glms
  sl_gam_t, # stacked gams
  Lrnr_glmnet$new(),
  Lrnr_cox,
  Lrnr_KM,
  Lrnr_glmnet$new(formula = ~ . + treatment + t + treatment * t),
   Lrnr_mean$new()
)


# learning ensemble for censoring time hazard
stack.censoring <- stack.failure

learner.event_type <- stack.failure

# learning ensemble for propensity score and event type distribution (if competing risks)
# learning ensemble
sl_glm_oneway <- Lrnr_sl$new(learners = lapply(covariates, function(var) {
  var <- c(var)
  Lrnr_glm$new(  covariates = var, family = binomial())
}), metalearner = Lrnr_nnls$new(),
fold_fun = origami::folds_vfold, V = 5)

sl_gam <- Lrnr_sl$new(learners = lapply(covariates, function(var) {
  var <- c(var)
  Lrnr_gam$new(covariates = var, family = binomial())
}
), metalearner = Lrnr_nnls$new(),
fold_fun = origami::folds_vfold, V = 5)

learner.treatment <- Stack$new(
  sl_glm_oneway, sl_gam, Lrnr_mean$new(),
  Lrnr_glmnet$new(),   Lrnr_bayesglm$new())


threshold_list <- sort(unique(data[[marker]][data_surv[[event_type]]==1]))
threshold_list <- unique(quantile(threshold_list, seq(0, 1, length = 20), type = 1))
print(table(data_surv[[marker]] >= threshold))
data_surv <- copy(data_surv)
treatment <- "treatment"
out_list <- list()
learner.treatment <- stack.failure <- stack.censoring <- learner.event_type <- Lrnr_gam$new()

stack.failure <- stack.censoring <- Lrnr_hal9001$new(formula = ~ h(., k =1, s = 1) + h(t, k = 20, pf = 0, s = 0), smoothness_orders = 1)
 
 
  
 
for(threshold in threshold_list  ) {
  data_surv[[treatment]] <- 1*(data_surv[[marker]] >= threshold)
  if(sum(1*(data_surv[[marker]] >= threshold)) > 30) {
  #print(mean(1*(data_surv[[marker]] >= threshold)))
  survout <- survtmle3_discrete(data_surv[[failure_time]], data_surv[[event_type]],
                                data_surv[[treatment]], data_surv[, covariates, with = FALSE],
                                weights = data_surv[[weights]],
                                learner.treatment =  learner.treatment,
                                learner.failure_time =  stack.failure,
                                learner.censoring_time = stack.censoring,
                                learner.event_type = learner.event_type,
                                target_failure_time = tf_discrete,
                                target_treatment = c(1),
                                target_event_type = 1,
                                failure_time.stratify_by_time = FALSE,
                                censoring_time.stratify_by_time = FALSE,
                                cross_fit = FALSE,
                                cross_validate = FALSE,
                                calibrate = FALSE,
                                verbose = TRUE, max_iter = 100)
   
  out_list[[paste0(threshold)]] <- survout
  }
    
}
 
 

```



# Competing risk analysis 

 
```{r}


library(data.table)
library(assertthat)
# # relevant variables
# subset_region <- "la"
# weights_twostage <- "wt.D29" #"wt.D29start1"
# covariates <- c("standardized_risk_score", "Sex") #c("HighRiskInd", "Sex", "age.geq.65","standardized_risk_score")
# failure_time <- "EventTimePrimaryD29"
# event_type <- "EventIndPrimaryMolecConfirmedD29"
# viral_load <- "seq1.log10vl"
# marker <- paste0("Day29pseudoneutid50", subset_region)
# variant_type <- "seq1.variant.hotdeck1"
# Perprotocol <- "Perprotocol"    
# TwophasesampIndD29 <- "TwophasesampIndD29"
# Trt <- "Trt"
# # get data
 
 

run_covid_analysis = function(subset_region = "la",
weights_twostage = "wt.D29",
covariates = c("standardized_risk_score", "Sex"),
failure_time = "EventTimePrimaryD29",
event_type = "EventIndPrimaryMolecConfirmedD29",
viral_load = "seq1.log10vl",
marker =  "Day29pseudoneutid50",
variant_type = "seq1.variant.hotdeck1",
Perprotocol = "Perprotocol"    ,
TwophasesampIndD29 = "TwophasesampIndD29",
Trt = "Trt"
){
data <- setDT(fread(paste0("~/repositories/covidanalysis/data/janssen_", subset_region, "_partA_data_processed_with_riskscore_hotdeckv4.csv")))
subset <- which(data[[Perprotocol]] == 1)
data <- data[subset]


 

data <- data[, c(weights_twostage, marker, event_type, failure_time, covariates, variant_type, Perprotocol, TwophasesampIndD29, Trt , viral_load, "EventIndPrimaryD29"), with = FALSE]
# make competing risk indicators
variant_strata <- c("Ancestral.Lineage" = 181 ,
                           "Zeta" = 176,
                           "Lambda" = 77,
                           "Mu" = 175,
                           "Gamma" = 181
                           )
variant_names <- names(variant_strata)
for(variant in variant_names) {
  print(variant)
  event_type_key <- paste0(event_type, "_", variant_type, "_", variant )
  value <- data[[event_type]]
  value[is.na(value)] <- -1
  value[!is.na(value) & value==1] <- ifelse(data[[variant_type]][!is.na(value) & value==1] == variant, 1, 2)
  # Any remaining NAS are assigned a competing risk
  value[value == -1] <- NA
  value[is.na(value)] <- 2
  data[, (event_type_key) := value]
 


 
# use competing weights
weights <- weights_twostage

# Run competing risk analysis
tf <- variant_strata[paste0(variant)]
event_type_target <- event_type_key
# for CR only, remove observations without variant information
#if(event_type != event_type_target) {
 # data <- data[!is.na(data[[variant_type]])]
#}
 
# make datasets of placebo and treated analysis
subset_treated <- data[[Trt]]==1
data_placebo <- data[!subset_treated]
data_treated <- data[subset_treated]
subset <- which(data_treated[[TwophasesampIndD29]] == 1)
data_treated <- data_treated[subset]  # assumes TwophasesampIndD29 used only for treatment arm
# subset to reelvant variables
data_treated <- data_treated[, c(covariates, failure_time, event_type_target, marker, weights), with = FALSE]
data_placebo <-  data_placebo[, c(covariates, failure_time, event_type_target), with = FALSE]
assert_that(all(
  data_treated$Trt == 1 
  & data_treated$Perprotocol == 1
  & data_treated$TwophasesampIndD29 == 1
  & !any(is.na(data_treated[[variant_type]]))
))
assert_that(all(
  data_placebo$Trt == 0 
  & data_placebo$Perprotocol == 1
  & data_placebo$TwophasesampIndD29 == 1
  & !any(is.na(data_placebo[[variant_type]]))
))


form <- as.formula(paste0("Surv(", failure_time,  ", as.factor(", event_type_target, ")", ") ~ 1"))
data_placebo <- as.data.table(na.omit(data_placebo))
fit <- cmprsk::cuminc(data_placebo[[failure_time]], data_placebo[[event_type_target]])
time_index <- min(which.min(abs(fit$`1 1`$time - tf)))
est <- fit$`1 1`$est[time_index]
se <- sqrt(fit$`1 1`$var[time_index])


    

#### Analysis for treated
## get reference time for survival analysis
# output_treated <- run_analysis(tf = tf, 
#              data_surv = data_treated, 
#              covariates = covariates, 
#              failure_time = failure_time,
#              event_type = event_type_target, 
#              weights = weights, 
#              marker = marker,
#              nbins_time = 20,
#              nbins_threshold = 20)
# 
# # make survival dataset
#  output_treated <- output_treated[-ncol(output_treated)]
#  output_treated$estimates <- unlist( output_treated$estimates )
#   output_treated$times <- unlist( output_treated$times )
#   output_treated$estimates_monotone <- -as.stepfun(isoreg(output_treated$threshold, -output_treated$estimates))(output_treated$threshold)
#   
#   fwrite(output_treated, paste0("~/repositories/covidanalysis/results/vaccine_", marker, "_", failure_time, "_", event_type_target, ".csv"))
# }
}
}
```


 # plotting
 
```{r}
subset_region <- "la"
marker <- paste0("Day29pseudoneutid50", subset_region)
failure_time <- "EventTimePrimaryD29"
 weights <- "wt.D29"
 event_type <- "EventIndPrimaryMolecConfirmedD29"
 variant_type <- "seq1.variant.hotdeck4"
 variant <- "Lambda"
event_type_target <- paste0(event_type, "_", variant_type, "_", variant )

 file <- paste0("vaccine_", marker, "_", failure_time, "_", event_type_target, ".csv")
 output_treated <- fread(paste0("~/repositories/covidanalysis/results/", file))

 library(ggplot2)
 
 
data <- setDT(fread(paste0("~/repositories/covidanalysis/data/janssen_", subset_region, "_partA_data_processed_with_riskscore_hotdeckv4.csv")))
subset <- which(data[[Perprotocol]] == 1 & data$Trt == 1)
data <- data[subset]

 
 data_tmp <- na.omit(data[, c(marker, weights), with = FALSE])
  scale_coef <- max(plot$data$estimates, na.rm = T) * 1
  RCDF <- function(a) {
    sum(data_tmp[[weights]] * (data_tmp[[marker]] >= a)) / sum(data_tmp[[weights]]) * scale_coef
  }
  RCDF <- Vectorize(RCDF)
 laby <- "hi"
  
 
  plot <- ggplot(output_treated, aes(x = threshold, y = estimates_monotone)) + geom_point(color = "blue") + geom_line(color = "darkgrey") +  theme_bw() +scale_x_continuous(breaks = round(output_treated$threshold,2)) + labs(x = marker, y = event_type_target)
 
  
  
  
  col <- c(col2rgb("olivedrab3")) # orange, darkgoldenrod2
  col <- rgb(col[1], col[2], col[3], alpha = 255 * 0.4, maxColorValue = 255)
   main <- "title"
  plot <- plot + ggtitle(main) +
    stat_function(fun = RCDF, color = col, geom = "area", fill = col, alpha = 0.2) +
    scale_y_continuous(
      name = laby,
      sec.axis = sec_axis(~ . / scale_coef, name = "Reverse CDF"), n.breaks = 10
    )  +  theme(axis.text.x = element_blank(), axis.ticks = element_blank()) #+
  #    theme(plot.title = element_text(size = 25), axis.text.x = element_text(angle = 0, hjust = 1, size = 18), axis.text.y = element_text(angle = 0, hjust = 1, size = 18))+ 
  # geom_text(alpha = 0.75, aes(quantile(plot$data$cutoffs, 0.1),min(max(plot$data$upper),risk_plac),label = paste0("placebo overall risk: ", risk_plac)), vjust = 0, size = 5) + 
  #   scale_x_continuous(
  # breaks = xx,
  #  labels = do.call(expression,labels),
  #  name =labx,
  #  limits = xlimits
  # 
  # )
  # plot <- plot + geom_vline(xintercept = max_thresh, colour = "red", linetype = "longdash")
 
plot
 
```


```{r}
library(future)
library(sl3)
plan(multisession, workers = 6)

run_analysis <- function(tf, data_surv, covariates, failure_time, event_type, weights , marker = NULL, nbins_time = 20, nbins_threshold = 20) {
  data_surv <- as.data.table(data_surv)
   # effectivelly removed observations with no weights
  data_surv <- na.omit(data_surv[, c(covariates, failure_time, event_type, marker, weights), with = FALSE ])
  # discretize
  time_grid <- unique(quantile(data_surv[[failure_time]], seq(0,1, length = nbins_time+1), type = 1))
  failure_time_discrete <- findInterval(data_surv[[failure_time]], time_grid, all.inside = TRUE)
  tf_discrete <- findInterval(tf, time_grid, all.inside = FALSE)
  data_surv[[failure_time]] <-failure_time_discrete
  
  
  # time-dependent learners
  # -- Superlearners of single covariate glm (one-way and two-wau)
  sl_glm_oneway_t <- Lrnr_sl$new(learners = lapply(covariates, function(var) {
    var <- c(var, "treatment", "t")
    Lrnr_glm_fast$new(formula =  ~ . , covariates = var)
  }), Lrnr_nnls$new())
  sl_glm_twoway_t <- Lrnr_sl$new(learners = lapply(covariates, function(var) {
    var <- c(var, "treatment", "t")
    Lrnr_glm_fast$new(formula =  ~ . + t*treatment, covariates = var)
  }), metalearner = Lrnr_nnls$new(), 
  fold_fun = origami::folds_vfold, V = 5)
  # superlearner of single covariate GAMS
  sl_gam_t <- Lrnr_sl$new(learners = lapply(covariates, function(var) {
    var <- c(var, "treatment", "t")
    Lrnr_gam$new(covariates = var)
  }
  ), metalearner = Lrnr_nnls$new(),
  fold_fun = origami::folds_vfold, V = 5)
  # Kaplan Meiier using HAL
  Lrnr_KM <- Lrnr_hal9001$new(formula = ~  h(t, k = 20, pf = 0, s = 0), smoothness_orders = 0)
  # COX using HAL
  Lrnr_cox <- Lrnr_hal9001$new(formula = ~ h(., k =1, s = 1) + h(t, k = 20, pf = 0, s = 0), smoothness_orders = 1)
  # baseline learners
  sl_glm_oneway <- Lrnr_sl$new(learners = lapply(covariates, function(var) {
    var <- c(var)
    Lrnr_glm$new(  covariates = var, family = binomial())
  }), metalearner = Lrnr_nnls$new(),
  fold_fun = origami::folds_vfold, V = 5)
  sl_gam <- Lrnr_sl$new(learners = lapply(covariates, function(var) {
    var <- c(var)
    Lrnr_gam$new(covariates = var, family = binomial())
  }
  ), metalearner = Lrnr_nnls$new(),
  fold_fun = origami::folds_vfold, V = 5)
  
  
  
  
  # learning ensemble for censoring time hazard
  stack.failure <- Stack$new(
    sl_glm_oneway_t, # stacked one-way glms
    sl_glm_twoway_t, # stacked two-way glms
    sl_gam_t, # stacked gams
    Lrnr_glmnet$new(),
    Lrnr_cox,
    Lrnr_KM,
    Lrnr_glmnet$new(formula = ~ . + treatment + t + treatment * t),
    Lrnr_mean$new()
  )
  stack.censoring <- stack.failure
  learner.event_type <- stack.failure
  learner.treatment <- Stack$new(
    sl_glm_oneway, sl_gam, Lrnr_mean$new(),
    Lrnr_glmnet$new(),   Lrnr_bayesglm$new())
  
  
  
  # TODO
  learner.treatment <- stack.failure <- stack.censoring <- learner.event_type <- Lrnr_gam$new()
  stack.failure <- stack.censoring <- Lrnr_hal9001$new(formula = ~ h(., k =1, s = 1) + h(t, k = 20, pf = 0, s = 0), smoothness_orders = 1)
  data_surv <- copy(data_surv)
  treatment <- "treatment"
  # if no marker, then just get overall risk estimae
  if(is.null(marker)) {
    data_surv$treatment <- 1
    survout <- survtmle3_discrete(data_surv[[failure_time]], data_surv[[event_type]],
                                  data_surv[[treatment]], data_surv[, covariates, with = FALSE],
                                  weights = data_surv[[weights]],
                                  learner.treatment =  Lrnr_glmnet$new(),
                                  learner.failure_time =   Lrnr_glmnet$new(),
                                  learner.censoring_time =  Lrnr_glmnet$new(),
                                  learner.event_type =  Lrnr_glmnet$new(),
                                  target_failure_time = tf_discrete,
                                  target_treatment = c(1),
                                  target_event_type = 1,
                                  failure_time.stratify_by_time = FALSE,
                                  censoring_time.stratify_by_time = FALSE,
                                  cross_fit = FALSE,
                                  cross_validate = FALSE,
                                  calibrate = FALSE,
                                  verbose = TRUE, max_iter = 100)
  } else {
    
    # if marker, tun threshold analysis
    all_thresholds <- as.vector(na.omit(data[[marker]][data_surv[[event_type]] != 0]))
    #threshold_list <- seq(min(all_thresholds), quantile(all_thresholds, 0.95), length = nbins_threshold)
    threshold_list <- min(all_thresholds)
    threshold_list <- sort(unique(
      c(
        threshold_list,
        quantile(setdiff(all_thresholds, threshold_list), seq(0, 0.95, length = nbins_threshold), type = 1)
        )))
     
    
    treatment <- "treatment"
    out_list <- list()
    for(threshold in threshold_list  ) {
      data_surv[[treatment]] <- 1*(data_surv[[marker]] >= threshold)
      if(sum(data_surv[[marker]] >= threshold) >= 30) {
        #print(mean(1*(data_surv[[marker]] >= threshold)))
        survout <- survtmle3_discrete(data_surv[[failure_time]], data_surv[[event_type]],
                                      data_surv[[treatment]], data_surv[, covariates, with = FALSE],
                                      weights = data_surv[[weights]],
                                      learner.treatment =  Lrnr_glmnet$new(),
                                      learner.failure_time =  Lrnr_glmnet$new(),
                                      learner.censoring_time = Lrnr_glmnet$new(),
                                      learner.event_type = Lrnr_glmnet$new(),
                                      target_failure_time = tf_discrete,
                                      target_treatment = c(1),
                                      target_event_type = 1,
                                      failure_time.stratify_by_time = FALSE,
                                      censoring_time.stratify_by_time = FALSE,
                                      cross_fit = FALSE,
                                      cross_validate = FALSE,
                                      calibrate = FALSE,
                                      verbose = TRUE, max_iter = 100)
        
        survout$threshold <- threshold
        out_list[[paste0(threshold)]] <- survout
      }
      
    }
    output <- rbindlist(out_list)
    return(output)
  }
}

out_list[[1]]
 

```



# survtmle check

```{r}
# run survtmle3 for discrete time (check without weights)

survout <- survtmle3_discrete(data_surv[[failure_time]], data_surv[[event_type]],
                              data_surv[[treatment]], data_surv[, covariates, with = FALSE],
                              weights = data_surv[[weights]],
                              learner.treatment = sl_glm_oneway,
                              learner.failure_time =  stack.failure,
                              learner.censoring_time = stack.censoring,
                              learner.event_type = learner.event_type,
                              target_failure_time = tf_discrete,
                              target_treatment = c(1),
                              target_event_type = 1,
                              failure_time.stratify_by_time = FALSE,
                              censoring_time.stratify_by_time = FALSE,
                              cross_fit = TRUE,
                              calibrate = FALSE,
                              verbose = TRUE)
library(SuperLearner)
david_out <- survtmle::survtmle(data_surv[[failure_time]], data_surv[[event_type]],
                                data_surv[[treatment]], data_surv[, covariates, with = FALSE],
                                wts = data_surv[[weights]],
                                SL.ftime = "SL.glmnet",
                                SL.ctime = "SL.glmnet",
                                SL.trt = "SL.glmnet",
                                t0 = tf_discrete,
                                tol = 1/sqrt(nrow(data_surv)) / log(nrow(data_surv)),
)
as.vector(unlist(survout$estimates))
as.vector(david_out$est[2,])
unlist(survout$se)
sqrt(diag(david_out$var))[2]
```
